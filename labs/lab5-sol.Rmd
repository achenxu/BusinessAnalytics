---
title: "Business Analytics; Lab 5"
author: "Souhaib Ben Taieb and Shin Tan"
date: "22 and 23 August 2016"
output: html_document
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
```
# The bootstrap

### Assignment - Question 1

Do the exercise 2 in Section 5.4 of ISLR.

### Bootstrap confidence interval of the correlation coefficient

We will find a 95% confidence interval for the correlation coefficient of Median House value and average number of rooms in the Boston data set from the `MASS` package.

```{r}
library(ISLR)
library(ggplot2)
```

The functions `cor` and `cor.test` will compute the correlation and an asymptotic 95% confidence interval for it. This interval is based on Fisher's z transform
$$ z = \frac{1}{2}\log\left(\frac{1+r}{1-r}\right)$$
which is approximately normally distributed with variance $1/(n-3)$ where $n$ is the number of observations. So if $z_L$ and $z_U$ are upper and lower limits for $z$, then 
$$
r_L = \frac{\exp(2z_L) - 1}{\exp(2z_L)+1}
  \qquad\text{and}\qquad 
r_U = \frac{\exp(2z_U) - 1}{\exp(2z_U)+1}
$$
are upper and lower limits for $r$.

We will use the bootstrap to test if this is a good approximation in this case.

### Exercise 1

Check that the confidence interval returned by `cor.test` is computed using the above transformation. 

```{r}
library(MASS)

n <- nrow(Boston)
r <- cor(Boston$medv, Boston$rm)

# Fisher interval
cor.test(Boston$medv, Boston$rm)

z <- 0.5*log((1+r)/(1-r))
zint <- z + 1.96/sqrt(n-3)*c(-1,1)
rint <- (exp(2*zint)-1)/(exp(2*zint)+1)
```

### Exercise 2

Compute a 95% bootstrap confidence interval for the correlation. You will need to sample rows of the `Boston` matrix.

```{r}
B <- 1000
rb <- numeric(B)
for(i in 1:B)
{
  bootstrapdata <- Boston[sample(n, replace=TRUE),]
  rb[i] <- cor(bootstrapdata$medv, bootstrapdata$rm)
}
quantile(rb, prob=c(0.025,0.975))
```

### Assignment - Question 2

Write a function that will return a bootstrap confidence interval for the correlation of any two numeric variables of the same length. Your function should take four arguments: 

 - `x`: a numeric vector of data
 - `y`: a numeric vector of data
 - `level`: the probability coverage of the confidence interval with default value of 0.95 
 - `B`: the number of bootstrap samples with default value of 1000.

```{r}
bootstrap.cor.int <- function(x, y, level=0.95, B=1000)
{
  n <- length(x)
  rb <- numeric(B)
  for(i in 1:B)
  {
    j <- sample(n, replace=TRUE)
    rb[i] <- cor(x[j],y[j])
  }
  alpha = 1-level
  return(quantile(rb, prob=c(alpha/2, 1-alpha/2)))
}

bootstrap.cor.int(Boston$medv,Boston$rm,B=10000)
```


# Dimension reduction with PCA

### Assignment - Question 3

We will run PCA on the multiple test scores for Australian 15 year olds [PISA test scores](en.wikipedia.org/wiki/Programme_for_International_Student_Assessment). Download [data set](http://bsouhaib.github.io/BusinessAnalytics/labs/PISA-oz.csv) and [documentation](http://bsouhaib.github.io/BusinessAnalytics/labs/PISA-oz-dict.csv)


## Task 1

Read in the PISA data. How many students were tested? How many variables are included in the data set? Read the data dictionary to find out what the variables named ST08Q01 PV1MACC  PV2MACC  PV3MACC PV4MACC  PV5MACC  PV1MACQ  PV2MACQ  PV3MACQ  PV4MACQ PV5MACQ PV1MACS  PV2MACS  PV3MACS PV4MACS  PV5MACS  PV1MACU  PV2MACU  PV3MACU  PV4MACU  PV5MACU  PV1MAPE  PV2MAPE  PV3MAPE  PV4MAPE  PV5MAPE  PV1MAPF  PV2MAPF PV3MAPF  PV4MAPF  PV5MAPF  PV1MAPI  PV2MAPI  PV3MAPI  PV4MAPI  PV5MAPI are. Write a couple of sentences  describing them.

```{r, echo=FALSE, message=FALSE, results='hide'}
oz <- read.csv("../data/PISA-oz.csv") # You might need to change the directory to where the data is located on your computer
dim(oz)
```

*14,481 Australian students were tested in 2012. This data set has 80 variables.*

*ST08Q01 is the gender of the student*

*PV1MACC-PV5MACC are measuring understanding of change and relationships, PV1MACQ-PV5MACQ measure understanding od quantity, 
PV1MACS-PV5MACS, measure space and shape, PV1MACU-PV5MACU measure uncertainty and data, perhaps the closest to statistics, PV1MAPE-PV5MAPE measure employ, which we would guess to be run the ideas, PV1MAPF-PV5MAPF cover formulating the problems, and PV1MAPI-PV5MAPI tests interpretative skills.*

## Task 2

Compute a PCA on the variables PV1MACC through PV5MAPI. Make a scree plot, and examine the principal components for the first 4. What proportion of variation in the data is explained by the first principal component? Second, third and fourth? 

```{r, echo=FALSE, message=FALSE, results='hide', fig.show='hide'}
library(dplyr)
math <- select(oz, PV1MACC:PV5MAPI)
math_pca <- prcomp(math, scale=T, retx=T)
plot(math_pca, type="l")
options(digits=2)
math_pca$sdev
math_pca$rotation[,1:4]
```

*The proportion of variation explained by PC1 is* `r math_pca$sdev[1]^2/35`, and `r math_pca$sdev[2]^2/35`, `r math_pca$sdev[3]^2/35` and `r math_pca$sdev[4]^2/35`, *for the second, third and fourth respectively.* 

## Task 3 

Compute the average for each student for each of the different types of math tasks. Based on the PCA explain why this would be a reasonable thing to do. Make a scatterplot matrix of the average scores.

```{r, echo=FALSE, message=FALSE, results='hide', fig.show='hide'}
math_ave <- data.frame(MACC=apply(math[,1:5], 1, mean), 
                       MACQ=apply(math[,6:10], 1, mean), 
                       MACS=apply(math[,11:15], 1, mean), 
                       MACU=apply(math[,16:20], 1, mean), 
                       MAPE=apply(math[,21:25], 1, mean), 
                       MAPF=apply(math[,26:30], 1, mean), 
                       MAPI=apply(math[,31:35], 1, mean)) 
library(ggplot2)
library(GGally)
ggscatmat(math_ave)
```

*The main source of variation in the data is the sum of all types of tests. The second and third principal components suggest the secondary source of variation is in the type of math test, because the coefficients break up into groups of 5 that match the different tests. This tells us that the test scores are pretty similar for type of test, and differ more across test types, so it is reasonable to average the scores for the type of test.*

## Task 4

Compute the average overall math score for each student (this means averaging PV1MATH-PV5MATH). Make a side-by-side boxplot of these scores by gender. Is there a difference in math scores for girls and boys? Write a few sentences explaining what you learn. (Note that the full range of math scores is 0-1000.)

```{r, echo=FALSE, message=FALSE, results='hide', fig.show='hide'}
m_g <- data.frame(gender=oz$ST04Q01, math=apply(oz[,31:35], 1, mean))
qplot(gender, math, data=m_g, geom="boxplot")
```

*The median math score for girls is a little lower than for boys, and the spread is a little smaller. The boys scores may be considered roughly to be shifted up by about 10 points from the girls scores. There is little difference in the two distributions because 10 points out of 1000 is very small. The top score was earned by a boy, but the second top by a girl, and the lowest score was earned by a boy.*

## Task 5 

How many different schools were included in the survey? Compute the average math score (average the averages) and standard deviation for each school, and make an ordered dotplot (with bars indicating one standard deviation above and below the mean) of these averages. Write a couple of sentences that describe how math scores vary across schools.

```{r, echo=FALSE, message=FALSE, results='hide', fig.show='hide', warning=FALSE}
length(unique(oz$SCHOOLID))
m_g <- data.frame(sch=oz$SCHOOLID, math=apply(oz[,31:35], 1, mean))
mga <- summarise(group_by(m_g, sch), m=mean(math), s=sd(math))
mga$sch <- factor(mga$sch, levels=order(mga$m))
qplot(sch, m, data=mga) + 
  geom_linerange(aes(x=sch, ymin=m-s, ymax=m+s)) + coord_flip()
```

*There is a difference of about 600 points from the average of the top school to that of the bottom. The standard deviations are reasonably similar, on the order of 100 points. The data suggests that the school does matter, and that schools in Australia are heterogeneous in the math scores the students earn.*


## TURN IN 

- Your `.Rmd` file (which should knit without errors and without assuming any packages have been pre-loaded)
- Your Word (or pdf) file that results from knitting the Rmd.
- DUE: 29 August 10am (late submissions not allowed), loaded into moodle
