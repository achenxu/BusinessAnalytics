---
title: "ETC3250 Lab 10"
author: "Di Cook"
date: "Week 10"
output: pdf_document
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  echo=FALSE,
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
```

```{r}
library(readr)
library(ggplot2)
library(tidyr)
library(dplyr)
library(lubridate)
library(GGally)
library(rpart)
library(rpart.plot)
library(randomForest)
library(e1071)
library(caret)
library(gridExtra)
```

## Purpose

In this lab we will fit a variety of classifiers including penalized LDA and xgboost and compare the performance, for a high dimension, low sample size data set.

## Data

This lab will examine the happy paintings by Bob Ross, using several different classifiers.  The paintings were the subject of the [538 post](http://fivethirtyeight.com/features/a-statistical-analysis-of-the-work-of-bob-ross/), "A Statistical Analysis of the Work of Bob Ross".

We have taken the painting images from the [sales site](http://www.saleoilpaintings.com/paintings/bob-ross/bob-ross-sale-3_1.html), read the images into R, and resized them all to be 20 by 20 pixels. Each painting has been classified into one of 8 classes based on the title of the painting. This is the data that you will work with.
In wide form, each row corresponds to one painting, and the rgb color values at each pixel are in each column. With a $20\times20$ image, this leads to $400\times3=1200$ columns.

Here are three of the original paintings in the collection, labelled as "scene", "water", "flowers":

\centerline{\includegraphics[width=2in]{../data/bobross140.jpg}
\includegraphics[width=2in]{../data/bobross167.jpg}}

```{r fig.width=8, fig.height=2.5}
paintings_long <- read_csv("../data/paintings-long-train.csv")
df <- filter(paintings_long, id == 140)
p2 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
df <- filter(paintings_long, id == 167)
p3 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
grid.arrange(p2, p3, ncol=3)
```

The data has been subsetted to contain just two classes: flowers and cold. The files are `paintings_training_sub.csv` and `paintings_test_sub.csv`.

## Reading

Do a little reading about the xgboost algorithm. Here is a starting place:
[R-bloggers explanation of the xgboost algorithm](https://www.r-bloggers.com/an-introduction-to-xgboost-r-package/)

```{r results='hide', fig.show='hide'}
tr <- read_csv("../data/paintings_training_sub.csv")
ts <- read_csv("../data/paintings_test_sub.csv")
```

## Question 1

a. Build a linear discriminant analysis model to predict whether the painting is about flowers or cold theme. 
b. Compute the error of the model for the test data.
c. Summarise the coefficients of the LDA classifier.

```{r results='hide', fig.show='hide'}
library(MASS)
tr_lda <- lda(class~., data=tr[,-c(1,2)], prior=c(0.5,0.5))
```

## Question 2

a. Build a penalised linear discriminant analysis model to predict whether the painting is about flowers or cold theme. 
b. Compute the error of the model for the test data.
c. Summarise the coefficients of the Penalised LDA classifier.
d. Discuss how these differ fomr the LDA coefficients.

```{r results='hide', fig.show='hide'}
library(penalizedLDA)
cls <- ifelse(tr[,3]=="flowers", 2, 1)
set.seed(1)
tr_plda <- PenalizedLDA(as.matrix(tr[,-c(1:3)]), cls, 
                        as.matrix(ts[,-c(1:3)]), lambda=0.001, K=1)
table(ts$class, tr_plda$ypred)
```

## Question 3

a. Build a support vector machine to predict whether the painting is about flowers or cold theme. 
b. Compute the error of the model for the test data.

```{r results='hide', fig.show='hide'}
library(e1071)
tr_svm <- svm(tr[,-c(1:3)], cls, kernel="linear")
psvm <- round(predict(tr_svm, ts[,-c(1:3)]), 0)
table(ts$class, psvm)
```

## Question 4

a. Build a random forest classifier to predict whether the painting is about flowers or cold theme. 
b. Compute the error of the model for the test data.
c. Compare the ten most important variables from random forest with that of penalizedLDA. Is there much overlap in the subset of variables?

```{r results='hide', fig.show='hide'}
library(randomForest)
tr$class <- factor(tr$class)
tr_rf <- randomForest(tr[,-c(1:3)], tr$class, ntree=1000)
prf <- predict(tr_rf, ts, type="class")
table(ts$class, prf)
```

## Question 5

a. Write a paragraph describing the xgboost algorithm, in your own words.
b. Build an xgboost model to predict whether the painting is about flowers or cold theme. 
c. Compute the error of the model for the test data.
d. Tweak the inputs to predict the test as best as you can.

```{r results='hide', fig.show='hide'}
library(xgboost)
dtrain <- xgb.DMatrix(as.matrix(tr[,-c(1:3)]), label = cls)
param <- list(max.depth = 2, eta = 1, silent = 1)
tr_xgb <- xgb.train(param, dtrain, nthread = 2, nround = 10)
pxgb <- round(predict(tr_xgb, as.matrix(ts[,-c(1:3)])), 0)
table(ts$class, pxgb)
```

## Question 6

Write a couple of paragraphs to compare and contrast the different classifiers for building a model on the paintings data.

## WHAT TO TURN IN

Turn in two items: a `.Rmd` document, and the output `.pdf` or `.docx` from running it. No need to include the R output in your output, but the code should be in the Rmd file. Include your plots in your output.
