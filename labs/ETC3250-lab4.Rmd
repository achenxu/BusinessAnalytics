---
title: "ETC3250 Lab 4"
author: "Rob Hyndman"
date: "18 August 2015"
output: html_document
---

## Writing your own cross-validation function

Last week we used a function `CV()` that computed the LOOCV statistic using the neat computational trick available for linear models based on the hat-matrix.

This week you will write your own function `myCV()` that will compute the same statistic, but do it the long way by fitting models to different sets of training data.

myCV() should work for linear regression models and take two arguments:

```{r eval=FALSE}
myCV <- function(formula, data)
{

}
```

You can apply the model `formula` to the data set leaving out the $i$th observation like this:
```{r eval=FALSE}
fit <- lm(formula, data=data[-i,])
```

To get the prediction of the omitted observation, use the predict function:
```{r eval=FALSE}
pred <- predict(fit, newdata=data[i,])
```

The response value for the omitted observation can be found as follows.
```{r eval=FALSE}
responsevar <- as.character(formula(fit))[2]
data[i,responsevar]
```

You will need to use a `for` loop to cycle through all the values of `i`.

To test you have done it correctly, check that `myCV()` and `CV()` return the same value.

## k-fold cross-validation

Now add a new argument to your `myCV()` function to allow it to do k-fold cross-validation. The `k` argument can take values between 2 and `n` where `n=nrow(data)`. For example `myCV(formula, data, k=5)` will do a 5-fold cross-validation. When `k=n`, the function should do leave-one-out cross-validation. 

You should shuffle your data frame before doing the division of data into folds.

You can check that your updated function works correctly by comparing the results against `CV()` for different values of `k`. When `k=n`, the results should be identical. For other values of `k`, the results should be close but not identical.
