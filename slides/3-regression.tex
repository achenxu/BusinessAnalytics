% * Week 3. (Aug 10) Regression for prediction. Ch3 (Rob)
%   - Lecture 5: Review of linear regression, matrix formulation
%   - Lab 3:
%   - Lecture 6: Subset selection, LOOCV 


\documentclass[14pt]{beamer}
\usepackage{pgf,tikz,pgfpages,amsmath,bm,fancyvrb,animate}
\usepackage{graphicx,bera,booktabs}
\usepackage[australian]{babel}
\usepackage[utf8]{inputenc}

\usetheme{Monash}
\def\biz{\begin{itemize}[<+-| alert@+>]}
\def\eiz{\end{itemize}}
\def\ben{\begin{enumerate}[<+-| alert@+>]}
\def\een{\end{enumerate}}

\def\E{\text{E}}
\def\V{\text{Var}}

\graphicspath{{../figures/}{../figures/book_figures/Chapter3/}{../figures/book_figures/Chapter7/}}

\title[3. Regression]{Business Analytics}
\author{3. Regression}
\date{10 August 2015}

\DefineShortVerb{\"}
\def\FancyVerbFormatCom{\color[rgb]{0.6,0,1}\relax}


\begin{document}

\begin{frame}[plain]{}
\maketitle
\begin{textblock}{11}(0.5,1.19){\color{white}\large
\textbf{ETC3250}}
\end{textblock}
\end{frame}

\section{Revision: multiple regression}


\begin{frame}{Revision: Multiple regression}

\begin{block}{}\vspace*{-0.2cm}
$$Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \cdots + \beta_pX_{p,i} + e_i.$$
\end{block}
\biz
\item Each $X_{j,i}$ is numerical and is called a ``predictor''.

\item The coefficients $\beta_1,\dots,\beta_p$ measure the effect of each
predictor after taking account of the effect of all other predictors
in the model.


\item Predictors may be transforms of other predictors. e.g., $X_2=X_1^2$.

\item The model describes a line, plane or hyperplane in the predictors.


\eiz

\end{frame}

\begin{frame}{Revision: Multiple regression}
\only<1>{\fullheight{3-1}}
\only<2>{\fullheight{3-5}}
\end{frame}

\begin{frame}{Revision: Multiple regression}\large
\begin{itemize}[<+-| alert@+>]
\item Dummy variables
\item OLS
\item $R^2$
\item se of coefficients
\item residual standard error
\item F statistic
\end{itemize}
\end{frame}

\begin{frame}{Important questions}\large 
\begin{enumerate}
\item Is at least one of the predictors useful in predicting the response?

\item Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?

\item How well does the model fit the data?

\item Given a set of predictor values, what response value should we predict and how accurate is our prediction?

\end{enumerate}

\end{frame}

\begin{frame}{Credit scores}

Banks score loan customers based on a lot of personal information. A sample of 500 customers from an Australian bank provided the following information.\vspace*{.2cm}

\hspace*{-0.3cm}{\footnotesize
\begin{tabular}{rrrrr}
  \hline
\bf Score & \bf Savings & \bf Income & \bf Time current address & \bf Time current job \\
& \$'000 & \$'000 & Months & Months \\
  \hline
39.40 & 0.01 & 111.17 &  27 &   8 \\
  51.79 & 0.65 & 56.40 &  29 &  33 \\
  32.82 & 0.75 & 36.74 &   2 &  16 \\
  57.31 & 0.62 & 55.99 &  14 &   7 \\
  37.17 & 4.13 & 62.04 &   2 &  14 \\
  33.69 & 0.00 & 43.75 &   7 &   7 \\
  25.56 & 0.94 & 79.01 &   4 &  11 \\
  32.04 & 0.00 & 45.41 &   3 &   3 \\
  41.34 & 4.26 & 55.22 &  16 &  18 \\
  \vdots & \vdots & \vdots & \vdots & \vdots\\
%  26.72 & 0.01 & 55.03 &  10 &  32 \\
%   \hline
\end{tabular}
}

\only<2>{\begin{textblock}{9}(3,5)
\begin{block}{}Can we use only savings, income, time @ address and time employed to predict the score of a new customer?\end{block}
\end{textblock}}

\end{frame}

\begin{frame}{Credit scores}
\only<1>{\placefig{4.4}{1.2}{height=8.cm,trim=0 0 0 0,clip=TRUE}{scores1}}
\only<2>{\placefig{4.4}{1.2}{height=8.cm,trim=0 0 0 0,clip=TRUE}{scores2}
\begin{textblock}{4}(0,1.5)\small
\biz
\item Taking logarithms reduces the skewness in the predictor variables.
\item Because of zeros, I used $\log(x+1)$.
\eiz
\end{textblock}}
\end{frame}

\begin{frame}{Credit scores}

\structure{Proposed model}
\begin{block}{}
\centerline{$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + e$}
\end{block}
\begin{align*}
\text{where~~}
Y     &=  \text{Credit score},         \\
X_{1} &=  \text{log savings}, \\
X_{2} &=  \text{log income},      \\
X_{3} &=  \text{log time at current address},\\
X_4     &=  \text{log time in current job},\\
e     &=  \text{error}.
\end{align*}



\end{frame}

\begin{frame}[fragile]{Credit scores}\footnotesize
\begin{verbatim}
lm(formula = score ~ log.savings + log.income + log.address + 
    log.employed, data = creditlog)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)    -0.219      5.231   -0.04   0.9667
log.savings    10.353      0.612   16.90  < 2e-16
log.income      5.052      1.258    4.02  6.8e-05
log.address     2.667      0.434    6.14  1.7e-09
log.employed    1.314      0.409    3.21   0.0014

Residual standard error: 10.2 on 495 degrees of freedom
Multiple R-squared: 0.47, Adjusted R-squared: 0.466 
F-statistic:  110 on 4 and 495 DF,  p-value: <2e-16 

    CV  AIC AICc  BIC   AdjR2
 104.7 2326 2326 2351 0.46582
\end{verbatim}
\vspace*{10cm}
\end{frame}


\begin{frame}{\large Interpreting regression coefficients}

\begin{itemize}
\item The ideal scenario is when the predictors are uncorrelated.
\begin{itemize}
\item Each coefficient can be interpreted and tested separately.
\end{itemize}
\item Correlations amongst predictors cause problems.
\begin{itemize}
\item The variance of all coefficients tends to increase, sometimes dramatically.
\item Interpretations become hazardous -- when $X_j$ changes, everything else changes.
\item Predictions still work provided new $X$ values are within the range of training $X$ values.
\end{itemize}
\item Claims of causality should be avoided for observational data.
\end{itemize}

\end{frame}

\begin{frame}{Interactions}
\begin{itemize}
\item An interaction occurs when the one variable changes the effect of a second variable. (e.g., spending on radio advertising increases the effectiveness of TV advertising).

\item To model an interaction, include the product $X_1X_2$ in the model in addition to $X_1$ and $X_2$.

\item \structure{Hierarchy principle}: If we include an interaction in a model, we should also include the main effects, even if the p-values associated
with their coefficients are not significant. (This is because the interactions are almost impossible to interpret without the main effects.)
\end{itemize}

\end{frame}




\begin{frame}{Residual patterns}

\biz
\item If a plot of the residuals vs any predictor in the model shows a pattern, then the relationship is nonlinear.

\item If a plot of the residuals vs any predictor \textbf{not} in the model shows a pattern, then the predictor should be added to the model.

\item If a plot of the residuals vs fitted values shows a pattern, then there is heteroscedasticity in the errors. (Could try a transformation.)

\eiz
\end{frame}



\section{Matrix formulation}

\begin{frame}{Matrix formulation}

\begin{block}{}\vspace*{-0.3cm}
$$Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \cdots + \beta_pX_{p,i} + e_i.$$
\end{block}\pause

Let $\bm{Y} = (Y_1,\dots,Y_n)'$, $\bm{e} = (e_1,\dots,e_n)'$, $\bm{\beta} = (\beta_0,\dots,\beta_p)'$ and
\[
\bm{X} = \begin{bmatrix}
  1 & X_{1,1} & X_{2,1} & \dots & X_{p,1}\\
  1 & X_{1,2} & X_{2,2} & \dots & X_{p,2}\\
\vdots & \vdots & \vdots & & \vdots\\
  1 & X_{1,n} & X_{2,n} & \dots & X_{p,n}
  \end{bmatrix}.
\]\pause

Then
\begin{block}{}\vspace*{-0.35cm}
$$\bm{Y} = \bm{X}\bm{\beta} + \bm{e}.$$
\end{block}
\end{frame}

\begin{frame}{Matrix formulation}

\structure{Least squares estimation}

Minimize: $(\bm{Y} - \bm{X}\bm{\beta})'(\bm{Y} - \bm{X}\bm{\beta})$\pause
\vspace*{0.5cm}

Differentiate wrt $\bm{\beta}$ gives

\begin{block}{}\vspace*{-0.2cm}
\[
\hat{\bm{\beta}} = (\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y}
\]
\end{block}
\pause
(The ``normal equation''.)\pause

\[
\hat{\sigma}^2 = \frac{1}{n-k-1}(\bm{Y} - \bm{X}\hat{\bm{\beta}})'(\bm{Y} - \bm{X}\hat{\bm{\beta}})
\]
\vspace*{.2cm}

\structure{Note:} If you fall for the dummy variable trap, $(\bm{X}'\bm{X})$ is a singular matrix.
\end{frame}


\begin{frame}{Likelihood}

If the errors are iid and normally distributed, then
\[
\bm{Y} \sim \text{N}(\bm{X}\bm{\beta},\sigma^2\bm{I}).
\]\pause
So the likelihood is
\[
L = \frac{1}{\sigma^n(2\pi)^{n/2}}\exp\left(-\frac1{2\sigma^2}(\bm{Y}-\bm{X}\bm{\beta})'(\bm{Y}-\bm{X}\bm{\beta})\right)
\]\pause
which is maximized when $(\bm{Y}-\bm{X}\bm{\beta})'(\bm{Y}-\bm{X}\bm{\beta})$ is minimized.\pause

\centerline{\textcolor[rgb]{0.80,0.00,0.00}{So \textbf{MLE = OLS}.}}
\end{frame}


\begin{frame}{Multiple regression predictions}

\begin{block}{Optimal predictions}\vspace*{-0.2cm}
\[
\hat{Y}^* =
\text{E}(Y^* | \bm{Y},\bm{X},\bm{X}^*) =
\bm{X}^*\hat{\bm{\beta}} = \bm{X}^*(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y}
\]
\end{block}
where $\bm{X}^*$ is a row vector containing the values of the regressors for the predictions (in the same format as $\bm{X}$).\vspace*{0.cm}

\pause

\begin{block}{Prediction variance}\vspace*{-0.2cm}
\[
\text{Var}(Y^* | \bm{Y},\bm{X},\bm{X}^*) = \sigma^2 \left[1 + \bm{X}^* (\bm{X}'\bm{X})^{-1} (\bm{X}^*)'\right]
\]
\end{block}\pause

\biz
\item This ignores any errors in $\bm{X}^*$.

\item 95\% prediction intervals assuming normal errors:
$  \hat{Y}^* \pm 1.96 \sqrt{\text{Var}(Y^*| \bm{Y},\bm{X},\bm{X}^*)}
$.
\eiz

\end{frame}




\section{Splines}




\begin{frame}{Splines}
\only<1>{\fullheight{draftspline}}
\only<2>{\fullwidth{spline}}
\only<3>{\fullwidth{spline2}}
\end{frame}
 
\begin{frame}{Splines}\parskip=1.6ex

Knots: $\kappa_1,\dots,\kappa_K$.

\pause

A spline is a continuous function $f(x)$ consisting of polynomials between each
consecutive pair of `knots' $x=\kappa_j$ and $x=\kappa_{j+1}$.

\pause

\begin{itemize}
\item Parameters constrained so that $f(x)$ is continuous.

\item Further constraints imposed to give continuous derivatives.
\end{itemize}
\end{frame}


\begin{frame}
\fullheight{7-3}

\end{frame}

\begin{frame}{Truncated power basis}
\biz
%\item
%Let $\kappa_1<\kappa_2<\cdots<\kappa_k$ be knots in interval $(a,b)$.

\item Predictors: $x$, \dots, $x^p$, $(x-\kappa_{1})_+^p$, \dots, $(x-\kappa_{K})_+^p$

\item Then the regression is piecewise order-$p$ polynomials.
\item $p-1$ continuous derivatives.
\item Usually choose $p=1$ or $p=3$.

\item $p+K+1$ degrees of freedom
\eiz
\end{frame}


\begin{frame}[fragile]{Natural splines}

\begin{itemize}
\item 
Splines based on truncated power bases have high variance at the outer range of the predictors.
\item Natural splines are similar, but have additional \alert{boundary constraints}: the function is linear at the boundaries. This reduces the variance.

\item Degrees of freedom $\verb|df|=K+1$\\ (i.e., there are $K=\verb|df|-1$ interior knots.)

\item Create predictors using \verb|ns| function in R (automatically chooses knots given \verb|df|).


\end{itemize}

\end{frame}

\begin{frame}{Natural splines}
\only<1>{\fullheight{7-4}}

\only<2>{\fullheight{7-7}}
\end{frame}

\begin{frame}{Basis functions}
\only<1>{\fullwidth{polybasis}}
\only<2>{\fullwidth{truncpolybasis}}
\only<3>{\fullwidth{nsbasis}}
\only<4>{\fullwidth{bsbasis}}
\end{frame}

\begin{frame}{Nonlinear choices}
\begin{enumerate}
\item Polynomials (beware)
\item Truncated power basis splines
\item Natural splines
\item B-splines
\item Smoothing splines
\item Kernel regression
\item Local regression
\item kNN
\end{enumerate}
\end{frame}



\end{document}