% * Week 3. (Aug 10) Regression for prediction. Ch3 (Rob)
%   - Lecture 5: Review of linear regression, matrix formulation
%   - Lab 3:
%   - Lecture 6: Subset selection, LOOCV 


\documentclass[14pt]{beamer}
\usepackage{pgf,tikz,pgfpages,amsmath,bm,fancyvrb,animate}
\usepackage{graphicx,bera,booktabs}
\usepackage[australian]{babel}
\usepackage[utf8]{inputenc}

\usetheme{Monash}
\def\biz{\begin{itemize}[<+-| alert@+>]}
\def\eiz{\end{itemize}}
\def\ben{\begin{enumerate}[<+-| alert@+>]}
\def\een{\end{enumerate}}

\def\E{\text{E}}
\def\V{\text{Var}}

\graphicspath{{../figures/}{../figures/book_figures/Chapter3/}{../figures/book_figures/Chapter7/}}

\title[2. Statistical learning]{Business Analytics}
\author{2. Statistical learning}


\DefineShortVerb{\"}
\def\FancyVerbFormatCom{\color[rgb]{0.6,0,1}\relax}


\begin{document}

\begin{frame}[plain]{}
\maketitle
\begin{textblock}{11}(0.5,1.19){\color{white}\large
\textbf{ETC3250}}
\end{textblock}
\end{frame}

\section{Revision: multiple regression}


\begin{frame}{Revision: Multiple regression}

\begin{block}{}\vspace*{-0.2cm}
$$Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \cdots + \beta_pX_{p,i} + e_i.$$
\end{block}
\biz
\item Each $X_{j,i}$ is numerical and is called a ``predictor''.

\item The coefficients $\beta_1,\dots,\beta_p$ measure the effect of each
predictor after taking account of the effect of all other predictors
in the model.

\item Recall: dummy variables, OLS, $R^2$, se of coefficients, residual standard error, F statistic.

\item Predictors may be transforms of other predictors. e.g., $X_2=X_1^2$.

\item The model describes a line, plane or hyperplane in the predictors.

\eiz

\end{frame}

\begin{frame}{Revision: Multiple regression}
\only<1>{\fullheight{3-1}}
\only<2>{\fullheight{3-5}}
\end{frame}


\begin{frame}{Important questions}\large 
\begin{enumerate}
\item Is at least one of the predictors useful in predicting the response?

\item Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?

\item How well does the model fit the data?

\item Given a set of predictor values, what response value should we predict and how accurate is our prediction?

\end{enumerate}

\end{frame}

\begin{frame}{Credit scores}

Banks score loan customers based on a lot of personal information. A sample of 500 customers from an Australian bank provided the following information.

\hspace*{-0.3cm}{\footnotesize
\begin{tabular}{rrrrr}
  \hline
\bf Score & \bf Savings & \bf Income & \bf Time current address & \bf Time current job \\
& \$'000 & \$'000 & Months & Months \\
  \hline
39.40 & 0.01 & 111.17 &  27 &   8 \\
  51.79 & 0.65 & 56.40 &  29 &  33 \\
  32.82 & 0.75 & 36.74 &   2 &  16 \\
  57.31 & 0.62 & 55.99 &  14 &   7 \\
  37.17 & 4.13 & 62.04 &   2 &  14 \\
  33.69 & 0.00 & 43.75 &   7 &   7 \\
  25.56 & 0.94 & 79.01 &   4 &  11 \\
  32.04 & 0.00 & 45.41 &   3 &   3 \\
  41.34 & 4.26 & 55.22 &  16 &  18 \\
  \vdots & \vdots & \vdots & \vdots & \vdots\\
%  26.72 & 0.01 & 55.03 &  10 &  32 \\
%   \hline
\end{tabular}
}

\only<2>{\begin{textblock}{9}(3,5)
\begin{block}{}Can we use only savings, income, time @ address and time employed to predict the score of a new customer?\end{block}
\end{textblock}}

\end{frame}

\begin{frame}{Credit scores}
\only<1>{\placefig{4.4}{1.2}{height=8.cm,trim=0 0 0 0,clip=TRUE}{scores1}}
\only<2>{\placefig{4.4}{1.2}{height=8.cm,trim=0 0 0 0,clip=TRUE}{scores2}
\begin{textblock}{4}(0,1.5)\small
\biz
\item Taking logarithms reduces the skewness in the predictor variables.
\item Because of zeros, I used $\log(x+1)$.
\eiz
\end{textblock}}
\end{frame}

\begin{frame}{Credit scores}

\structure{Proposed model}
\begin{block}{}
\centerline{$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + e$}
\end{block}
\begin{align*}
\text{where~~}
Y     &=  \text{Credit score},         \\
X_{1} &=  \text{log savings}, \\
X_{2} &=  \text{log income},      \\
X_{3} &=  \text{log time at current address},\\
X_4     &=  \text{log time in current job},\\
e     &=  \text{error}.
\end{align*}



\end{frame}

\begin{frame}[fragile]{Credit scores}\footnotesize
\begin{verbatim}
lm(formula = score ~ log.savings + log.income + log.address + 
    log.employed, data = creditlog)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)    -0.219      5.231   -0.04   0.9667
log.savings    10.353      0.612   16.90  < 2e-16
log.income      5.052      1.258    4.02  6.8e-05
log.address     2.667      0.434    6.14  1.7e-09
log.employed    1.314      0.409    3.21   0.0014

Residual standard error: 10.2 on 495 degrees of freedom
Multiple R-squared: 0.47, Adjusted R-squared: 0.466 
F-statistic:  110 on 4 and 495 DF,  p-value: <2e-16 

    CV  AIC AICc  BIC   AdjR2
 104.7 2326 2326 2351 0.46582
\end{verbatim}
\vspace*{10cm}
\end{frame}


\begin{frame}{\large Interpreting regression coefficients}

\begin{itemize}
\item The ideal scenario is when the predictors are uncorrelated.
\begin{itemize}
\item Each coefficient can be interpreted and tested separately.
\end{itemize}
\item Correlations amongst predictors cause problems.
\begin{itemize}
\item The variance of all coefficients tends to increase, sometimes dramatically.
\item Interpretations become hazardous -- when $X_j$ changes, everything else changes.
\item Predictions still work provided new $X$ values are within the range of training $X$ values.
\end{itemize}
\item Claims of causality should be avoided for observational data.
\end{itemize}

\end{frame}

\begin{frame}{Interactions}
\begin{itemize}
\item An interaction occurs when the one variable changes the effect of a second variable. (e.g., spending on radio advertising increases the effectiveness of TV advertising).

\item To model an interaction, include the product $X_1X_2$ in the model in addition to $X_1$ and $X_2$.

\item \structure{Hierarchy principle}: If we include an interaction in a model, we should also include the main effects, even if the p-values associated
with their coefficients are not significant. (This is because the interactions are almost impossible to interpret without the main effects.)
\end{itemize}

\end{frame}




\begin{frame}{Residual patterns}

\biz
\item If a plot of the residuals vs any predictor in the model shows a pattern, then the relationship is nonlinear.

\item If a plot of the residuals vs any predictor \textbf{not} in the model shows a pattern, then the predictor should be added to the model.

\item If a plot of the residuals vs fitted values shows a pattern, then there is heteroscedasticity in the errors. (Could try a transformation.)

\eiz
\end{frame}



\section{Matrix formulation}

\begin{frame}{Matrix formulation}

\begin{block}{}\vspace*{-0.3cm}
$$Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \cdots + \beta_pX_{p,i} + e_i.$$
\end{block}\pause

Let $\bm{Y} = (Y_1,\dots,Y_n)'$, $\bm{e} = (e_1,\dots,e_n)'$, $\bm{\beta} = (\beta_0,\dots,\beta_p)'$ and
\[
\bm{X} = \begin{bmatrix}
  1 & X_{1,1} & X_{2,1} & \dots & X_{p,1}\\
  1 & X_{1,2} & X_{2,2} & \dots & X_{p,2}\\
\vdots & \vdots & \vdots & & \vdots\\
  1 & X_{1,n} & X_{2,n} & \dots & X_{p,n}
  \end{bmatrix}.
\]\pause

Then
\begin{block}{}\vspace*{-0.35cm}
$$\bm{Y} = \bm{X}\bm{\beta} + \bm{e}.$$
\end{block}
\end{frame}

\begin{frame}{Matrix formulation}

\structure{Least squares estimation}

Minimize: $(\bm{Y} - \bm{X}\bm{\beta})'(\bm{Y} - \bm{X}\bm{\beta})$\pause
\vspace*{0.5cm}

Differentiate wrt $\bm{\beta}$ gives

\begin{block}{}\vspace*{-0.2cm}
\[
\hat{\bm{\beta}} = (\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y}
\]
\end{block}
\pause
(The ``normal equation''.)\pause

\[
\hat{\sigma}^2 = \frac{1}{n-k-1}(\bm{Y} - \bm{X}\hat{\bm{\beta}})'(\bm{Y} - \bm{X}\hat{\bm{\beta}})
\]
\vspace*{.2cm}

\structure{Note:} If you fall for the dummy variable trap, $(\bm{X}'\bm{X})$ is a singular matrix.
\end{frame}


\begin{frame}{Likelihood}

If the errors are iid and normally distributed, then
\[
\bm{Y} \sim \text{N}(\bm{X}\bm{\beta},\sigma^2\bm{I}).
\]\pause
So the likelihood is
\[
L = \frac{1}{\sigma^n(2\pi)^{n/2}}\exp\left(-\frac1{2\sigma^2}(\bm{Y}-\bm{X}\bm{\beta})'(\bm{Y}-\bm{X}\bm{\beta})\right)
\]\pause
which is maximized when $(\bm{Y}-\bm{X}\bm{\beta})'(\bm{Y}-\bm{X}\bm{\beta})$ is minimized.\pause

\centerline{\textcolor[rgb]{0.80,0.00,0.00}{So \textbf{MLE = OLS}.}}
\end{frame}


\begin{frame}{Multiple regression predictions}

\begin{block}{Optimal predictions}\vspace*{-0.2cm}
\[
\hat{Y}^* =
\text{E}(Y^* | \bm{Y},\bm{X},\bm{X}^*) =
\bm{X}^*\hat{\bm{\beta}} = \bm{X}^*(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y}
\]
\end{block}
where $\bm{X}^*$ is a row vector containing the values of the regressors for the predictions (in the same format as $\bm{X}$).\vspace*{0.cm}

\pause

\begin{block}{Prediction variance}\vspace*{-0.2cm}
\[
\text{Var}(Y^* | \bm{Y},\bm{X},\bm{X}^*) = \sigma^2 \left[1 + \bm{X}^* (\bm{X}'\bm{X})^{-1} (\bm{X}^*)'\right]
\]
\end{block}\pause

\biz
\item This ignores any errors in $\bm{X}^*$.

\item 95\% prediction intervals assuming normal errors:
$  \hat{Y}^* \pm 1.96 \sqrt{\text{Var}(Y^*| \bm{Y},\bm{X},\bm{X}^*)}
$.
\eiz

\end{frame}




\section{Splines}




\begin{frame}{Splines}
\only<1>{\fullheight{draftspline}}
\only<2>{\fullwidth{spline}}
\only<3>{\fullwidth{spline2}}
\end{frame}
 
\begin{frame}{Splines}\parskip=1.6ex

Knots: $\kappa_1,\dots,\kappa_K$.

\pause

A spline is a continuous function $f(x)$ consisting of polynomials between each
consecutive pair of `knots' $x=\kappa_j$ and $x=\kappa_{j+1}$.

\pause

\begin{itemize}
\item Parameters constrained so that $f(x)$ is continuous.

\item Further constraints imposed to give continuous derivatives.
\end{itemize}
\end{frame}


\begin{frame}
\fullheight{7-3}

\end{frame}

\begin{frame}{Truncated power basis}
\biz
%\item
%Let $\kappa_1<\kappa_2<\cdots<\kappa_k$ be knots in interval $(a,b)$.

\item Predictors: $x$, \dots, $x^p$, $(x-\kappa_{1})_+^p$, \dots, $(x-\kappa_{K})_+^p$\pause

\item Then the regression is piecewise order-$p$ polynomials.
\item $p-1$ continuous derivatives.
\item Usually choose $p=1$ or $p=3$.

\item $p+K+1$ degrees of freedom
\eiz
\end{frame}


\begin{frame}[fragile]{Natural splines}

\begin{itemize}
\item 
Splines based on truncated power bases have high variance at the outer range of the predictors.
\item Natural splines are similar, but have additional \alert{boundary constraints}: the function is linear at the boundaries. This reduces the variance.

\item Degrees of freedom $\verb|df|=K+1$\\ (i.e., there are $K=\verb|df|-1$ interior knots.)

\item Create predictors using \verb|ns| function in R (automatically chooses knots given \verb|df|).


\end{itemize}

\end{frame}

\begin{frame}{Natural splines}
\only<1>{\fullheight{7-4}}

\only<2>{\fullheight{7-7}}
\end{frame}

\begin{frame}{Nonlinear choices}
\begin{enumerate}
\item Polynomials (beware)
\item Truncated power basis splines
\item Natural splines
\item B-splines
\item Smoothing splines
\item Kernel regression
\item Local regression
\item kNN
\end{enumerate}
\end{frame}

\section{Choosing regression variables}

\begin{frame}{Choosing regression models}
\biz
\item When there are many predictors, how should we choose which ones to use?

\item How do we choose the df for a spline?

\item We need a way of comparing two competing models

\item If there are a limited number of predictors, we can study all possible models.

\item Otherwise we need a search strategy to explore some potential models.
\eiz

\end{frame}


\begin{frame}{Choosing regression variables}


\structure{What not to do!}
\biz
\item Plot $Y$ against a particular predictor ($X_j$)
       and if it shows no noticeable relationship,  drop it.

%\item Look  at  correlations among the predictors
%       (all of the potential candidates)  and every time a large
%       correlation is encountered,  remove one of  the  two
%       variables from further consideration.

\item Do a multiple linear regression on all the predictors
      and
      disregard all variables whose  $p$ values are greater than 0.05.

\item Maximize $R^2$

\item Minimize SSE.
\eiz
\end{frame}%

\begin{frame}{Comparing regression models}
\structure{Sum of squared errors}
\[\text{SSE} = \sum_{i=1}^n e_i^2 \]
Minimizing SSE will always choose the model with the most predictors.

\pause
\structure{Estimated residual variance}
\[
\hat{\sigma}^2 = \frac{\text{SSE}}{n-k-1}
\]
where $k=$ no.\ predictors.\pause

Minimizing $\hat{\sigma}^2$ works quite well for choosing predictors (but better methods to follow).

\end{frame}


\begin{frame}{Comparing regression models}

However \dots
\biz
\item $R^2$  does not allow for ``degrees of \rlap{freedom''.}

\item Adding \textit{any} variable tends to increase the value of $R^2$, even if that variable is
\rlap{irrelevant.}
\eiz\pause

To overcome   this problem, we can use \rlap{\emph{adjusted $R^2$}:}
\begin{block}{}
\[
\bar{R}^2 = 1-(1-R^2)\frac{n-1}{n-k-1}
\]
\end{block}
\pause

\centerline{\textcolor[rgb]{0.8,0.00,0.00}{\textbf{Maximizing $\bar{R}^2$ is equivalent to minimizing $\hat\sigma^2$.}}}

%$\bar{R}^2$    is  referred  to  as
%``adjusted $R^2$'' or ``$R$-bar-squared,''    or sometimes as  ``$R^2$,
%corrected for degrees of freedom.''
\end{frame}


\begin{frame}{Test sets}
\placefig{0.5}{1.5}{width=12cm}{cv1}

\only<2>{\begin{textblock}{12}(0.5,2.9)
\structure{Leave one-out cross-validation (LOOCV)}
\end{textblock}}
\only<2>{\placefig{0.5}{3.5}{width=12cm}{cvloout}}

\only<3>{\begin{textblock}{12}(0.5,2.9)
\structure{5-fold cross-validation}
\end{textblock}}
\only<3>{\placefig{0.5}{3.5}{width=12cm}{cv2}}
\end{frame}

\begin{frame}{Cross-validation}

Leave-one-out cross-validation for regression can be carried out using the following steps.
\ben
\item Remove observation $i$ from the data set, and fit the model using the remaining data. Then compute the error ($e_i^*=y_i-\hat{y}_i$) for the omitted observation.
\item Repeat step 1 for $i=1,\dots,n$.
\item Compute the MSE from $\{e_1^*,\dots,e_n^*\}$. We shall call this the CV.
\een\pause
The best model is the one with the smallest value of CV.
\end{frame}

\begin{frame}{Multiple regression prediction}


\begin{block}{Fitted values}\vspace*{-0.2cm}
\[
\hat{\bm{Y}} =
\text{E}(\bm{Y}|\bm{X}) =
\bm{X}\hat{\bm{\beta}} = \bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y} = \bm{H}\bm{Y}
\]
\end{block}
where $\bm{H} = \bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'$ is the ``hat matrix''.\pause

\structure{Leave-one-out residuals}

Let $h_1,\dots,h_n$ be the diagonal values of $\bm{H}$, then the cross-validation statistic is
\begin{block}{}
\centerline{$\displaystyle
\text{CV} = \frac1n\sum_{i=1}^n[e_i/(1-h_i)]^2,$}
\end{block}
where $e_i$ is the residual obtained from fitting the model to all $n$ observations.


\end{frame}

\begin{frame}{Akaike's Information Criterion}

\begin{block}{}
\[
\text{AIC} = -2\log(L) + 2(k+1)
\]
\end{block}
where $L$ is the likelihood and $k$ is the number of predictors in the model.\pause

\biz
\item This is a \emph{penalized likelihood} approach.

\item \emph{Minimizing} the AIC gives the best model for prediction.

\item AIC penalizes terms more heavily than $\bar{R}^2$.

\item Minimizing the AIC is asymptotically equivalent to minimizing MSE via leave-one-out cross-validation.
\eiz

\end{frame}

\begin{frame}{Corrected AIC}

For small values of $n$, the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed.
\begin{block}{}
\[
\text{AIC}_{\text{C}} = \text{AIC} + \frac{2(k+2)(k+3)}{n-k-1}
\]
\end{block}
As with the AIC, the AIC$_{\text{C}}$ should be minimized.
\end{frame}

\begin{frame}{\normalsize Schwartz Bayesian Information Criterion}

\begin{block}{}
\[
\text{BIC} = -2\log(L) + (k+1)\log(n)
\]
\end{block}
where $L$ is the likelihood and $k$ is the number of predictors in the model.\pause

\biz
\item \emph{Minimizing} the BIC gives the best model for prediction.

\item BIC penalizes terms more heavily than \rlap{AIC}

\item Also called SBIC and SC.

\item Minimizing BIC is asymptotically equivalent to leave-$v$-out cross-validation when $v = n[1-1/(log(n)-1)]$.

\eiz

\end{frame}


\begin{frame}{Choosing regression variables}

\structure{Best subsets regression}

\biz
\item
Fit all possible regression models using one or more of the predictors.

\item
Choose the best model based on one of the measures of predictive ability (CV, AIC, AICc, BIC, $\bar{R}^2$).
\eiz

\vfill\pause

\structure{Warning!}
\begin{itemize}
\item If there are a large number of predictors, this is not possible.

For example, 44 predictors leads to 18 trillion possible models!
\end{itemize}

\end{frame}



\begin{frame}{Choosing regression variables}\footnotesize\vspace*{-0.2cm}
{\tabcolsep=0.1cm
\hspace*{-0.3cm}\begin{tabular}{ccccccccc}
  \hline
 Savings & Income & Address & Employed & CV & AIC & AICc & BIC & $\bar{R}^2$ \\
  \hline
  X & X & X & X & 104.7 & 2325.8 & 2325.9 & 2351.1 & 0.4658 \\
  X & X & X &   & 106.5 & 2334.1 & 2334.2 & 2355.1 & 0.4558 \\
  X &   & X & X & 107.7 & 2339.8 & 2339.9 & 2360.9 & 0.4495 \\
  X &   & X &   & 109.7 & 2349.3 & 2349.3 & 2366.1 & 0.4379 \\
  X & X &   & X & 112.2 & 2360.4 & 2360.6 & 2381.5 & 0.4263 \\
  X &   &   & X & 115.1 & 2373.4 & 2373.5 & 2390.3 & 0.4101 \\
  X & X &   &   & 116.1 & 2377.7 & 2377.8 & 2394.6 & 0.4050 \\
  X &   &   &   & 119.5 & 2392.1 & 2392.2 & 2404.8 & 0.3864 \\
    & X & X & X & 164.2 & 2551.6 & 2551.7 & 2572.7 & 0.1592 \\
    & X & X &   & 164.9 & 2553.8 & 2553.9 & 2570.7 & 0.1538 \\
    & X &   & X & 176.1 & 2586.7 & 2586.8 & 2603.6 & 0.0963 \\
    &   & X & X & 177.5 & 2591.4 & 2591.5 & 2608.3 & 0.0877 \\
    &   & X &   & 178.6 & 2594.6 & 2594.6 & 2607.2 & 0.0801 \\
    & X &   &   & 179.1 & 2595.3 & 2595.3 & 2607.9 & 0.0788 \\
    &   &   & X & 190.0 & 2625.3 & 2625.4 & 2638.0 & 0.0217 \\
    &   &   &   & 193.8 & 2635.3 & 2635.3 & 2643.7 & 0.0000 \\
   \hline
\end{tabular}}

\end{frame}


\begin{frame}{What to use?}

\structure{Choice: BIC, CV, AICc, AIC, $\bar{R}^2$.}
\pause
\biz
\item BIC tends to choose models too small for prediction.

\item As $n\rightarrow\infty$, BIC selects \emph{true} model if there is one.

\item $\bar{R}^2$ tends to select models too large.

\item AIC also slightly biased towards larger models (but only when $n$ is small).

\item Empirical studies AIC is better than BIC for prediction accuracy.
\eiz
\end{frame}

\begin{frame}[squeeze,shrink=0.99]{Choosing regression variables}
\structure{Backwards stepwise regression}

\biz
\item Start with a model containing all variables.

\item Try subtracting one variable at a time. Keep the model if it
has lower \rlap{CV, AICc or AIC.}

\item Iterate until no further improvement.
\eiz

\pause%\small

\structure{\textcolor[rgb]{0.80,0.00,0.00}{Notes:}}
\biz\itemsep=0cm\parskip=0cm
%\item Putting all the predictors in can lead to a significant
%\textit{overall effect} but with no way of determining the individual effects
%of each variable.
%
%\item Using stepwise regression is useful for choosing only the most
%significant variables in the regression model.

\item Stepwise regression is not guaranteed to lead to the best possible
model.

\item If you are trying several different models, use the CV, AICc or AIC %adjusted $R^2$
value to select between them.
\eiz

\end{frame}


\end{document}