---
title: 'ETC3250 Business Analytics: Advanced Classification - Regularization and Shrinkage'
author: "Souhaib Ben Taieb, Di Cook, Rob Hyndman"
date: "October 15, 2015"
output:
  beamer_presentation: 
    theme: Monash
---

## Problem

- When the number of variables $(p)$ is large, estimating a model is problematic.
- Particularly, when it is larger than the sample size ($p>>n$), the variance of an estimate could be $\infty$.
- Constraining, or shrinking the estimates, can substantially decrease the variance, while minimally affecting the bias.

## Simple solutions

- Subset selection: Fit models to best subset
- Dimension reduction: Use combinations of variables, e.g. PCs, and feed these into your model

## Shrinkage using Ridge Regression

- Modified least squares

$$\sum_{i=1}^n (y_i-b_0-\sum_{j=1}^p \beta_j x_{ij})^2 + \lambda\sum_{j=1}^p \beta_j^2$$

- where $\lambda$ is a tuning parameter
- Minimizing this quantity trades off error with small $\beta$'s, at least forcing some of them to be small

## Shrinkage using Lasso

- More recent alternative to ridge regression

$$\sum_{i=1}^n (y_i-b_0-\sum_{j=1}^p \beta_j x_{ij})^2 + \lambda\sum_{j=1}^p |\beta_j|$$

- the change using an $l_1$ error, really forces some of the coefficients to be 0.

## Simulation example

```{r echo=TRUE, error=FALSE, warning=FALSE, message=FALSE, fig.width=3, fig.height=3, fig.show='hide'}
x<-matrix(rnorm(20*100),ncol=100)
x[1:10,1]<-x[1:10,1]+5
x<-scale(x)
x<-data.frame(x, cl=c(rep("A",10),rep("B",10)))
library(ggplot2)
qplot(X1,X2,data=x,colour=cl, size=I(3), shape=cl) + 
  theme_bw() + theme(legend.position="None", aspect.ratio=1)
qplot(X2,X3,data=x,colour=cl, size=I(3), shape=cl) + 
  theme_bw() + theme(legend.position="None", aspect.ratio=1)

# Generate test data
x.t<-matrix(rnorm(10*100),ncol=100)
x.t[1:5,1]<-x.t[1:5,1]+5
x.t<-scale(x.t)
x.t<-data.frame(x.t, cl=c(rep("A",5),rep("B",5)))
```

## Simulation example

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
qplot(X1,X2,data=x,colour=cl, size=I(3), shape=cl) + 
  theme_bw() + theme(legend.position="None", aspect.ratio=1)
qplot(X2,X3,data=x,colour=cl, size=I(3), shape=cl) + 
  theme_bw() + theme(legend.position="None", aspect.ratio=1)
```

## Fit LDA

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
library(MASS)
x.lda<-lda(cl~., data=x[,c(1:2,101)], prior=c(0.5,0.5))
x.lda
```

## Predict LDA

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Plot training data and test in discriminant space

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Estimates

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
library(reshape)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Increase the number of noise variables

- The next few slides repeat the results just shown for increasing number of variables
- None of the additional variables contribute to the separation between classes
- Additional variables are purely noise

## Error

p=5

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:5,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Error

p=8

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:8,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Error

p=11

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:11,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Error

p=12

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:12,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Error

p=13

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:13,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Error

p=14

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:14,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Error

p=15

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:15,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Error

p=16

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:16,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
table(x$cl, x.lda.p$class)
x.t.p<-predict(x.lda, x.t)
table(x.t$cl, x.t.p$class)
```

## Plot training data and test in discriminant space

p=2 

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:2,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Plot training data and test in discriminant space

p=5 

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:5,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Plot training data and test in discriminant space

p=8 

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:8,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Plot training data and test in discriminant space

p=11 

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:11,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Plot training data and test in discriminant space

p=12 

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:12,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Plot training data and test in discriminant space

p=13 

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:13,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Plot training data and test in discriminant space

p=14 

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:14,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Plot training data and test in discriminant space

p=15 

```{r cache=FALSE, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:15,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
qplot(LD1, cl, data=data.frame(LD1=x.lda.p$x, cl=x$cl), ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-10,10)) +
  geom_point(data=data.frame(LD1=x.t.p$x, cl=x.t$cl), shape=2, size=5, colour="red")
```

## Estimates

p=2

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=3, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:2,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Estimates

p=5

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=3, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:5,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Estimates

p=8

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=3.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:8,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Estimates

p=11

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=4, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:11,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Estimates

p=12

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=4.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:12,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Estimates

p=13

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:13,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Estimates

p=14

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=5.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:14,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Estimates

p=15

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=6, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:15,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:15,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Estimates

p=16

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=6.5, fig.height=2.5, fig.show='hold'}
x.lda<-lda(cl~., data=x[,c(1:16,101)], prior=c(0.5,0.5))
x.lda.p<-predict(x.lda, x)
x.t.p<-predict(x.lda, x.t)
x.lda.m<-melt(abs(x.lda$scaling)/sqrt(sum(x.lda$scaling^2)))
x.lda.m$X1<-factor(x.lda.m$X1, levels=c(paste("X",1:16,sep="")))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.lda.m)),yend=abs(value), data=x.lda.m, size=I(2), ylim=c(0,1),xlab="Variable",ylab="Coefficient", geom="segment")
```

## Penalized LDA

- Reference: [http://faculty.washington.edu/dwitten/Papers/JRSSBPenLDA.pdf](http://faculty.washington.edu/dwitten/Papers/JRSSBPenLDA.pdf)
- LDA does an eigendecomposition of $W^{-1}B$, which creates an estimation problem if $<<p$
- Instead compute regularised versions of $W, B$

$$maximize_{\beta_k}{\beta^T_k} \hat{\Sigma}^k_b\beta_k -\lambda_k\sum_{j=1}^p|\hat{\sigma_k}\beta_{kj}|$$ subject to $$\beta^T_k \hat{\Sigma}_w\beta_k \leq 1, $$

where $\hat{\Sigma}^k_b = \frac{1}{n}X^TY(Y^TY)^{-1/2}P^\bot_k(Y^TY)^{-1/2}Y^TX$, $\hat{\Sigma}_w$ is a positive definite estimate of $\Sigma_w$.

## Penalized LDA

```{r echo=TRUE, error=FALSE, warning=FALSE, message=FALSE, fig.width=6.5, fig.height=2.5, fig.show='hold'}
library(penalizedLDA)
cv.out<-PenalizedLDA.cv(as.matrix(x[,c(1:15)]), as.numeric(x$cl), lambdas=c(1e-4,1e-3,1e-2,.1,1))
x.pda<-PenalizedLDA(as.matrix(x[,c(1:15)]), xte=as.matrix(x.t[,c(1:15)]), as.numeric(x$cl), lambda=cv.out$bestlambda, K=cv.out$bestK)
table(x.t$cl, x.pda$ypred)
```

## Plot training and test

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=2.5, fig.height=2.5, fig.show='hold'}
qplot(PD1, cl, data=data.frame(PD1=x.pda$xproj, cl=x$cl),
  ylab="Class",size=I(5),alpha=I(0.7), xlim=c(-5,5)) +
  geom_point(data=data.frame(PD1=x.pda$xteproj, cl=x.t$cl), shape=2, size=5,
  colour="red")
```

## Estimates

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.width=5.5, fig.height=2.5, fig.show='hold'}
x.pda.m<-melt(abs(x.pda$discrim))
qplot(x=X1, xend=X1, y=rep(0,nrow(x.pda.m)),yend=abs(value), data=x.pda.m, size=I(2), ylim=c(0,1), xlab="Variable",ylab="Coefficient", geom="segment")
```
