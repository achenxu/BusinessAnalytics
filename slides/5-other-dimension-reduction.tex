

\documentclass[14pt]{beamer}
\usepackage{pgf,tikz,pgfpages,amsmath,bm,fancyvrb,animate}
\usepackage{graphicx,bera,booktabs}
\usepackage[australian]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}

\usetheme{Monash}
\def\biz{\begin{itemize}[<+-| alert@+>]}
\def\eiz{\end{itemize}}
\def\ben{\begin{enumerate}[<+-| alert@+>]}
\def\een{\end{enumerate}}

\graphicspath{{../figures/}{../figures/book_figures/Chapter6/}}

\title[5. Other dimensionality reduction methods]{Business Analytics}
\author{Week 5\\ Other dimensionality reduction methods}


\DefineShortVerb{\"}
\def\FancyVerbFormatCom{\color[rgb]{0.6,0,1}\relax}


\begin{document}

\begin{frame}[plain]{}
\maketitle
\begin{textblock}{11}(0.5,1.3){\color{white}\large
\textbf{ETC3250}}
\end{textblock}
\end{frame}


\begin{frame}{Dimensionality reduction}\large

\begin{itemize}
\item Wy dimensionality reduction?
\begin{itemize}
	\item Curse of dimensionality
	\item Intrinsic dimensionality
	\item Visualization
\end{itemize}
\pause
\item Dimensionality reduction methods
\begin{itemize}
\item Feature selection vs feature extraction (examples? advantages?)
\item Unsupervised vs supervised
\item Linear vs nonlinear
\end{itemize}
\pause
\item Principal components analysis (PCA)
\begin{itemize}
\item PCA produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.
\item PCA: linear and unsupervised feature extraction
\end{itemize}

\end{itemize}


\end{frame}

%%%%%%%%%% PLS %%%%%%%%%%
\begin{frame}{\normalsize Dimensionality reduction in regression}


\begin{itemize}
	\item \textcolor{red}{How would you reduce dimensionality in linear regression?}
	\pause
	\item \textcolor{red}{How would use PCA to reduce dimensionality in linear regression?}
	\pause
	\item \emph{Principal components regression} (PCR): use PCA to construct the first $M$ principal components, $Z_1$, \dots, $Z_M$, and then fit a linear regression model using these components.
	\item Assumption: \emph{the directions in which $X_1$, \dots, $X_p$ show the most variation are the directions that are associated with $Y$}. \textcolor{red}{When is PCR better than traditional least squares?}
\end{itemize}

\end{frame}

\begin{frame}{Partial least squares}

\begin{itemize}
\item With PCR, the components are identified in an \textbf{unsupervised way}. No guarantee that the directions that \textcolor{blue}{best explain the predictors} will also be the \textcolor{blue}{best directions to use for predicting the response $Y$}. \textcolor{red}{How would you use the response $Y$ to reduce dimensionality?}
\pause
\item Partial least squares (PLS) is a \textbf{supervised} alternative to PCR. Same procedure as PCR, but the new features are identified in a supervised way. Roughly speaking, PLS attempts to find directions that help \textcolor{blue}{explain both the response and the predictors}.
%PLS seeks directions that have high variance and have high correlation with the response

\end{itemize}

\end{frame}

\begin{frame}{Partial least squares}\small

\begin{enumerate}
	\item Standardize the $p$ predictors
	\item Compute the first direction $Z_1$ by setting each $\phi_{j1}$ in $Z_1 = \sum_{j = 1}^p \phi_{j1} X_j$ equal to the coefficient from the simple linear regression of $Y$ onto $X_j$ (see Figure)
	\item For the second direction $Z_2$, we first adjust each of the variables for $Z_1$, by regressing each variable on $Z_1$, and taking residuals, say $X'_j$. These residuals represent the remaining information that has not been explained by the the first PLS direction $Z_1$. We then compute $Z_2$ using $X'_j$ as $Z_1$ was computed from $X_j$
	\item We can compute $Z_1, \dots, Z_M$ iteratively. Then we use least squares to fit a linear model using $Z_1, \dots, Z_M$ as for PCR. \textcolor{red}{How to choose $M$?}
\end{enumerate}

\end{frame}

\begin{frame}{PLS Example}

\fullwidth{6-21}

\begin{textblock}{12}(0.6,8.8)
\textcolor[RGB]{0,159,134}{Solid line: first PLS direction.}\qquad 
\textcolor[RGB]{0,159,134}{Dotted line: first PC.}
\end{textblock}
\end{frame}


\begin{frame}{Partial least squares}

\begin{itemize}
\item There are two variants of PLS: PLS1 (one response variable) and PLS2 (at least two response variables).
\item In practice, PLS1 is not better than ridge regression or PCR (PLS reduces bias but can potentially increase variance).
\item PLS2 is a useful tool for multiresponse regression.
\item Other supervised dimensionality reduction methods: CCA, LDA, etc.
\end{itemize}

\end{frame}

\begin{frame}{Multidimensional scaling}

Suppose that instead of measuring a $n \times p$ dataset $\bm{X} = [x_{ij}]$, we were only given a \textcolor{blue}{pairwise distance matrix} $\Delta$ where
$$ \Delta_{ij} = \lVert x_i - x_j \rVert, \quad i, j = 1, \dots, N$$
i.e. we do not know  the points themselves. \textcolor{red}{Can we find a lower-dimension representation $\bm{Z}$ for $\bm{X}$ from $\Delta$?} (If we have only a distance matrix, we cannot perform PCA.)

\pause

\emph{Multidimensional scaling} (MDS) attempts to find a lower dimensional space so that distances between points are preserved as well as possible. 

\end{frame}

\begin{frame}{Multidimensional scaling}

MDS seeks values $z_1, z_2, \dots, z_n \in \mathbb{R}^k$ that minimize the so-called stress function:

$$ S_M(z_1, z_2, \dots, z_n) = \sum_{i \neq j} (\lVert x_i - x_j \rVert - \lVert z_i - z_j \rVert)^2. $$
(least squares or \emph{Kruskal-Shephard} scaling).

A variation on least squares scaling minimizes
$$ S_{\text{Sm}}(z_1, z_2, \dots, z_n) = \sum_{i \neq j} \frac{(\lVert x_i - x_j \rVert - \lVert z_i - z_j \rVert)^2}{\lVert x_i - x_j \rVert} $$
(\emph{Sammmon mapping}) \textcolor{red}{Difference with least squares scaling?}

\end{frame}


%%%%%%%%%%%% Nonlinear dimension reduction methods  %%%%%%%%%%%%

\begin{frame}{Link between MDS and PCA}

Computation of PCs in PCA:
\begin{itemize}

\item Singular value decomposition

\begin{itemize}
\item $\bm{X} = \bm{U}\bm{\Lambda}\bm{V}'$ with $\bm{U}'\bm{U}=\bm{I}$ and $\bm{V}'\bm{V}=\bm{I}$
\item $\bm{\Phi} = \bm{V} \rightarrow \bm{Z} = \bm{X}\bm{\Phi}$
\end{itemize}
	
\item Eigenvalue decomposition

\begin{itemize}
\item $\bm{C} = \bm{X'X}$ where the columns of $\bm{X}$ are scaled
\item $\bm{C}=\bm{V}\bm{D}\bm{V}'$ with $\bm{V}'\bm{V}=\bm{I}$
\item $\bm{\Phi} = \bm{V} \rightarrow \bm{Z} = \bm{X}\bm{\Phi}$
\end{itemize}

\end{itemize}

%\vspace{.5cm}
%\centerline{\textcolor{blue}{PCA from distances only?}}

\end{frame}

\begin{frame}{Link between MDS and PCA} \small 

If $\Delta_{ij}$ are Euclidean distances between the rows of $\bm{X}$, then \textcolor{blue}{MDS is equivalent to PCA}.\\[.2cm]

Given distance matrix $\Delta \in \mathbb{R}^{n \times n}$,
\begin{enumerate}
	\item Recover the inner product $\bm{XX'}$ from $\Delta$
		\begin{itemize}
		\item Compute $A_{ij} = - \frac12 \Delta_{ij}^2$
		\item Double center $A$ to recover $B$: \\
		
			$B = (I - M) A (I - M)$ where $M = \frac1n \mathds{1}\mathds{1}' \in \mathbb{R}^{n \times n}$
		
		\end{itemize}
	\item Factorize $B$ to gest the first $k$ principal component scores
\end{enumerate}
This is called \emph{classical multidimensional scaling}.

$$ S_C(z_1, z_2, \dots, z_n) = \sum_{i \neq j} (\langle x_i - \bar x, x_j - \bar x\rangle - \langle z_i -\bar z, z_j -\bar z\rangle)^2$$

\end{frame}


\begin{frame}{Multidimensional scaling}

\begin{itemize}
\item Multidimensional scaling can be applied to any $\Delta_{ij}$, not just Euclidean distances
\item In this case, we don't compute principal component scores, and the lower-dimensional representation can be a nonlinear function of the data 
\item \textcolor{red}{When do we need to use non-Euclidean distances?}
\end{itemize}
\pause
\centerline{\includegraphics[width=.6\textwidth]{digits}}
We wish to remove the effect of rotation in measuring distances between two digits

% other distances

% objective function of MDS

% stree functions?

% Isomap (with graphs)

% Nonlinear dimensionality reduction 
\end{frame}

\begin{frame}{Isometric feature mapping}



\includegraphics[width=\textwidth]{isomap}

{\small (From Tenenbaum et al. (2000), ``A global geometric framework for nonlinear dimensionality reduction'')}

\begin{itemize}
\item Construct a graph $G = (V, E)$ based on the structure between $x_1, \dots, x_n$.
\item Then, define a graph distance $\Delta_{ij}^{\text{Isomap}}$ between $i$ and $j$, and use MDS for the low-dimensional representation
\end{itemize}



\end{frame}





\begin{frame}{\large Dimensionality reduction methods}

\begin{itemize}
\item Feature selection vs feature extraction
\item Linear and nonlinear
\item Unsupervised and supervised
\item Low-dimensional representation with maximum variance, that retains local properties of the data, etc.
\item \textbf{Linear PCA}, Nonlinear PCA, Kernel PCA, Sparse PCA, etc.
\item \textbf{MDS}, ICA, LDA, etc.
\item \textbf{PLS}, CCA, FA, etc.
\item \textbf{Isomap}, diffusion maps, MVU, LLE, t-SNE, autoencoders, etc.
\end{itemize}
 

\end{frame}




\end{document}