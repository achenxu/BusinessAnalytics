---
title: 'ETC3250 Business Analytics: Advanced Classification - Trees & Forests'
author: "Souhaib Ben Taieb, Di Cook, Rob Hyndman"
date: "October 5, 2015"
output:
  beamer_presentation: 
    theme: Monash
---

## Decision trees

- Recursive binary splitting 
- Compute all possible splits $(n-1)$ on every variable ($p$)
- Choose the best split of the data, the one that separates it into two groups which are the most "pure"
- Continue to operate on each of the subsets until a stopping criteria is satisfied (e.g. all cases in the subset are of one class, there are less than $m$ cases in a subset, ...)

## Example: olive oils

```{r olives, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, fig.show='hold', fig.height=3, fig.width=3}
library(ggplot2)
library(tourr)
library(dplyr)
library(tidyr)
library(scales)
library(rpart)
library(rpart.plot)
library(gridExtra)

data(olive)
olive$region <- factor(olive$region, labels=c("South", "Sardinia", "North"))
#qplot(eicosenoic, linoleic, data=olive, color=region, shape=region, 
#      alpha=I(0.8)) +
#  theme(aspect.ratio=1, legend.position="None")

olive.rp <- rpart(region~eicosenoic + linoleic, data=olive)
options(digits=2)
print(olive.rp)
table(olive$region, predict(olive.rp, olive, type="class"))
```

## Example: olive oils

```{r olive-fit, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, fig.height=2.5, fig.width=2.5}
prp(olive.rp, varlen=5)
```

## Example: olive oils

```{r olive-splits, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=3.5, fig.width=3.5}
qplot(eicosenoic, linoleic, data=olive, color=region, shape=region, 
      alpha=I(0.8)) +
  geom_vline(xintercept=6.5, color="black") +
  geom_segment(aes(x=0, xend=6.5, y=1054, yend=1054), color="black") + 
  theme(aspect.ratio=1, legend.position="None")
```

## Measuring the quality of splits

- Explanation for two classes (0,1), and $p$=proportion in class 0
- Entropy: $-p(log_e p)-(1-p)log_e(1-p)$
- Gini: $2p(1-p)$
- Misclassification: $1-2|p-0.5|$

## What these look like

```{r impurity, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=2.5, fig.width=3.5}
p <- seq(0, 1, 0.01)
ent <- scales::rescale(-p*log(p)-(1-p)*log(1-p))
gini <- scales::rescale(2*p*(1-p))
misc <- scales::rescale(1-2*abs(p-0.5))
df <- data.frame(p, ent, gini, misc)
df.m <- gather(df, impurity, value, -p)
qplot(p, value, data=df.m, color=impurity, geom="line")
```

## Choosing the best split

```{r splits, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=2.5, fig.width=3.5}
x <- sort(rnorm(9)*100)
y <- c(0,0,0,1,0,1,1,1,1)
df <- data.frame(x, y)
spl <- (x[1:8] + x[2:9])/2
qplot(x, y, data=df, color=factor(y), shape=factor(y)) + 
  geom_vline(xintercept=spl, linetype=2) +
  theme(legend.position="None") 
```

## Choosing the best split

- Calculate the impurity for each subset, and combine
- $p^L$ impurity $_L + p^R$ impurity$_R$

```{r splits2, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=2.5, fig.width=3.5}
calc.entropy <- function(x, cl) {
  ox <- order(x)
  n <- length(x)
  impurity <- NULL
  for (i in 1:(n-1)) {
    imp <- 0
    cnt <- 0
    cntL <- 0
    for (k in 1:i) {
      if (cl[ox[k]]==1) cntL <- cntL+1
    }
    cntR <- 0
    for (k in (i+1):n) {
      if (cl[ox[k]]==1) cntR <- cntR+1
    }
    pclL <- cntL/i
    pclR <- cntR/(n-i)
    if (pclL>0 & pclL<1)
      imp <- (i/n)*(-pclL*log(pclL) - (1-pclL)*log(1-pclL))
    if (pclR>0 & pclR<1)
      imp <- imp + ((n-i)/n)*(-pclR*log(pclR) - (1-pclR)*log(1-pclR))
#   cat(i,cntL,cntR,pclL,pclR,imp,i/n,(n-i)/n,"\n")
#    impurity<-rbind(impurity,c((x[ox[i]]+x[ox[i]+1])/2,imp,
#      (cl[ox[i]]+cl[ox[i]+1])/2))
    impurity <- rbind(impurity,c((x[ox[i]]+x[ox[i+1]])/2,imp))
  }     
  return(data.frame(x=impurity[,1], ent=impurity[,2]))
}
imp <- calc.entropy(df$x, df$y)
imp
```

## Choosing the best split

```{r splits3, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=2.5, fig.width=3.5}
df$yb <- 0
bestspl <- data.frame(x=imp$x[5], ent=imp$ent[5])
ggplot(data=df) + 
  geom_point(aes(x, yb, colour=factor(y), shape=factor(y))) + 
  geom_point(data=imp, mapping=aes(x=x, y=ent), colour=I("black"), 
             shape=I(1), size=3) + ylab("entropy") + 
  geom_vline(xintercept=spl, linetype=2) +
  geom_point(data=bestspl, aes(x=x, y=ent), colour="red") +
  geom_vline(xintercept=spl[5], colour="red") +
  theme(legend.position="None") 
```

## Example: olive oils

```{r olivesplits, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=2.5, fig.width=3.5}
olive.sub <- subset(olive, region != "South")
olive.sub$region <- factor(olive.sub$region, labels=c(0,1))
imp <- calc.entropy(olive.sub$linoleic, olive.sub$region)
qplot(x, ent, data=imp, geom="line") +
  geom_point(data=olive.sub, aes(x=linoleic, y=0, colour=region), alpha=0.4) +
  geom_vline(xintercept=1054, color="red") + ylab("entropy") + xlab("linoleic") +
  ylim(0, 0.7)
```

## Example: olive oils

```{r olivesplits2, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=5, fig.width=9}
imp <- calc.entropy(olive.sub$palmitic, olive.sub$region)
p1 <- qplot(x, ent, data=imp, geom="line") +  xlab("palmitic") +
  geom_point(data=olive.sub, aes(x=palmitic, y=0, colour=region), alpha=0.4) +
  ylab("") + ylim(0, 0.7) + theme(legend.position="None")
imp <- calc.entropy(olive.sub$palmitoleic, olive.sub$region)
p2 <- qplot(x, ent, data=imp, geom="line") +  xlab("palmitoleic") +
  geom_point(data=olive.sub, aes(x=palmitoleic, y=0, colour=region), alpha=0.4) +
  ylab("") + ylim(0, 0.7) + theme(legend.position="None")
imp <- calc.entropy(olive.sub$stearic, olive.sub$region)
p3 <- qplot(x, ent, data=imp, geom="line") +  xlab("stearic") +
  geom_point(data=olive.sub, aes(x=stearic, y=0, colour=region), alpha=0.4) +
  ylab("") + ylim(0, 0.7) + theme(legend.position="None")
imp <- calc.entropy(olive.sub$oleic, olive.sub$region)
p4 <- qplot(x, ent, data=imp, geom="line") +  xlab("oleic") +
  geom_point(data=olive.sub, aes(x=oleic, y=0, colour=region), alpha=0.4) +
  ylab("") + ylim(0, 0.7) + theme(legend.position="None")
imp <- calc.entropy(olive.sub$linoleic, olive.sub$region)
p5 <- qplot(x, ent, data=imp, geom="line") +  xlab("linoleic") +
  geom_point(data=olive.sub, aes(x=linoleic, y=0, colour=region), alpha=0.4) +
  ylab("") + ylim(0, 0.7) + theme(legend.position="None")
imp <- calc.entropy(olive.sub$linolenic, olive.sub$region)
p6 <- qplot(x, ent, data=imp, geom="line") +  xlab("linolenic") +
  geom_point(data=olive.sub, aes(x=linolenic, y=0, colour=region), alpha=0.4) +
  ylab("") + ylim(0, 0.7) + theme(legend.position="None")
imp <- calc.entropy(olive.sub$arachidic, olive.sub$region)
p7 <- qplot(x, ent, data=imp, geom="line") +  xlab("arachidic") +
  geom_point(data=olive.sub, aes(x=arachidic, y=0, colour=region), alpha=0.4) +
  ylab("") + ylim(0, 0.7) + theme(legend.position="None")
grid.arrange(p1, p2, p3, p4, p5, p6, p7, nrow=2, ncol=4)
```

## Stopping rules

- *minsplit:* minimum number of observations allowed in order to consider splitting
- *minbucket:* minimum number of observations in a terminal node
- *cp (0.01):* complexity parameter. The decrease in impurity cannot be less than this.

## Overfitting

- It is possible to force a tree to fit the training sample very closely, by tweaking these stopping rules.
- This could lead to overfitting, really small error with the training data and much higher error with validation data, and hence test data.
- Tuning the algorithm control parameters with the validation set is really important.

## Overfitting example

![figures/overfit1.pdf](../figures/overfit1.png)

## Overfitting example

![figures/overfit2.pdf](../figures/overfit2.png)

## Overfitting example

![figures/overfit3.pdf](../figures/overfit3.png)

## Overfitting example

![figures/overfit4.pdf](../figures/overfit4.png)

## Overfitting example

![figures/overfit5.pdf](../figures/overfit5.png)

## Overfitting example

![figures/overfit6.pdf](../figures/overfit6.png)

## Overfitting example

![figures/overfit7.pdf](../figures/overfit7.png)

## Overfitting example

![figures/overfit8.pdf](../figures/overfit8.png)

## Overfitting example

![figures/overfit9.pdf](../figures/overfit9.png)

## Overfitting example

![figures/overfit10.pdf](../figures/overfit10.png)

## Overfitting example

![figures/overfit11.pdf](../figures/overfit11.png)

## Overfitting example

![figures/overfit12.pdf](../figures/overfit12.png)

## Pruning

- Grow an overly complex tree
- Prune back the weakest branches, using the cost complexity, or cross-validation to get the lowest validation error

## Example: crabs

```{r crab, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=5, fig.width=9}
crab <- read.csv("http://www.ggobi.org/book/data/australian-crabs.csv")
crab <- subset(crab, species=="Blue", select=c("sex", "FL", "RW"))
crab.rp <- rpart(sex~FL+RW, data=crab, parms = list(split = "information"), 
                 control = rpart.control(minsplit=8))
crab.rp
```

## Example: crabs

```{r crab2, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=5, fig.width=9}
prp(crab.rp, varlen=6)
```

## Example: crabs

```{r crab3, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=5, fig.width=9}
qplot(FL, RW, data=crab, color=sex, shape=sex) + 
  theme(aspect.ratio=1) + 
  geom_vline(xintercept=16, linetype=3) + 
  geom_segment(aes(x=7, xend=16, y=12, yend=12), color="black", linetype=3) +
  geom_segment(aes(x=12, xend=12, y=12, yend=17), color="black", linetype=3) + 
  geom_segment(aes(x=7, xend=16, y=8.1, yend=8.1), color="black", linetype=3) +
  geom_segment(aes(x=11, xend=11, y=8.1, yend=12), color="black", linetype=3) +
  geom_segment(aes(x=11, xend=16, y=11, yend=11), color="black", linetype=3) +
  geom_segment(aes(x=11, xend=11, y=8.1, yend=11), color="black", linetype=3) +
  geom_segment(aes(x=12, xend=16, y=11, yend=11), color="black", linetype=3) +
  geom_segment(aes(x=14, xend=14, y=11, yend=12), color="black", linetype=3) +
  geom_segment(aes(x=16, xend=21.3, y=16, yend=16), color="black", linetype=3)
```

## Example: crabs

Complex tree

```{r crab-unpruned, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=5, fig.width=9}
crab.rp <- rpart(sex~FL+RW, data=crab, parms = list(split = "information"), 
                 control = rpart.control(minsplit=3))
prp(crab.rp)
```

## Example: crabs

Pruned tree

```{r crab-pruned, cache=FALSE, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, fig.height=5, fig.width=9}
crab.rp.prune <- prune(crab.rp, cp=0.05)
prp(crab.rp.prune)
```

## Advantages and disadvantages

- The decision rules provided by trees are very easy to explain, and follow. A simple classification model. 
- Trees can handle a mix of predictor types, categorical, quantitative, ....
- Trees efficiently operate when there are missing values in the predictors.
- 
- Algorithm is greedy, a better final solution might be obtained by taking a second best split earlier
- When separation is in linear combinations of variables trees struggle to provide a good classification

